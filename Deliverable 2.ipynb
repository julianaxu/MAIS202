{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juliana Xu"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Mushroom Classification - Deliverable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Problem Statment**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My project will aim to classify a given mushroom as either poisonous or safe to eat based on a given set of physical characteristics. Each mushroom is classified based on a maximum of 21 features that range from odor, to cap colour, to habitat. The classes are poisonous(1) and edible(0). The final conceptualization will be realized through a simple web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the following dataset: https://www.kaggle.com/uciml/mushroom-classification. The dataset consists of 23 labels and 8124 samples. Out of the 8124 samples, 48% percent are classified as poisonous and 52% as edible. Furthermore, the first label represents the class and the subsequent 22 represent features. However, I will be removing the label \"veil-type\" as 100% of the samples share the same veil type. There are also other features which exhibit similar behaviour, having nearly 100% of the samples sharing the same chracteristics. These labels include \"veil-color\", \"gill-attachment\", and \"ring-number\". I have considered removing these labels as well but decided against it as they could still be crucial to determining the edibility of a mushroom. Instead, I will be implementing the L1 regularization method which will automatically remove labels that have little to no effect on the final output.\n",
    "\n",
    "In addition to removing certain columns, I will need to convert the data in my dataset from categorical to numerical. This is to facilitate the process of implementing different classification algorithms. Currently, the dataset contains solely letters which are used to represent the class and features of each sample. To convert each letter into an integer I will be using the LabelEncoder() from sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Machine Learning Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my project which involves a classification problem, I will be testing out the Support Vector Classifier and the Random Forest Classifer. I will not need to consider the methods for multiclass problems since my project deals with a two class problem. I chose not to implement the k-NN model because my dataset contains a large number of features and the k-NN model is highly sensitive to higher dimensionalities. \n",
    "\n",
    "As for training/validation/test set splits, I plan on more or less following the standard. However, the large amount of features in my dataset may call for more samples in order obtain a comprehensive training phase. Therefore, I have decided to split my data 65%/15%/20% for the training, validation, and test sets, respectively. Also due to the numerous features, I am choosing to use the L1 regularization technique. As aforementioned, using this method will shrink the coefficients of less important features to zero, and therefore, eliminating them completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Preliminary Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor data (y_test):  ['p' 'p' 'p' ... 'e' 'p' 'e']\n",
      "Numerical data (y_test):  [1 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "mushrooms = pd.read_csv(\"/Users/julianaxu/Downloads/mushrooms.csv\")\n",
    "\n",
    "# Delete the colums with label \"veil-type\". Mushrooms now has shape (8124, 22).\n",
    "mushrooms = mushrooms.drop(\"veil-type\", axis = 1)\n",
    "\n",
    "# Create X and y. \n",
    "# y is an array of the first column. \n",
    "# X is an array of all the subsequent columns. \n",
    "X = np.array(mushrooms.iloc[:, 1:])\n",
    "y = np.array(mushrooms.iloc[:, 0])\n",
    "\n",
    "# Create training and test sets. Note: train/valid/test = .65/.15/.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "print(\"Factor data (y_test): \", y_test)\n",
    "\n",
    "# Create a validation set.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.15)\n",
    "\n",
    "# Use Label Encoder to turn categorical data into numerical.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "\n",
    "# Do the same for y_test. Note: 0 = edible, 1 = poisonous\n",
    "le.fit(y_test)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Do the same for y_valid.\n",
    "le.fit(y_valid)\n",
    "y_valid = le.transform(y_valid)\n",
    "\n",
    "# Use Label Encoder on X_train, X_valid and _test\n",
    "for i in range(21):\n",
    "    # X_train\n",
    "    le.fit(X_train[:, i])\n",
    "    X_train[:, i] = le.transform(X_train[:, i])\n",
    "    # X_valid\n",
    "    le.fit(X_valid[:, i])\n",
    "    X_valid[:, i] = le.transform(X_valid[:, i])\n",
    "    # X_test\n",
    "    le.fit(X_test[:, i])\n",
    "    X_test[:, i] = le.transform(X_test[:, i])\n",
    "    \n",
    "print(\"Numerical data (y_test): \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9476828385228095\n",
      "Test Accuracy:  0.9489230769230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Using SVM\n",
    "svm = LinearSVC(class_weight = 'balanced')\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_train, y_train)\n",
    "svm.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy: \", svm_clf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: \", svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SVM method, the accuracy is fairly high so it could be a good choice to use this classification technique. The test accuracy is higher than the training accuracy so I do not think there are any issues regarding underfitting or overfitting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Using RandomForests\n",
    "rfc = RandomForestClassifier(n_estimators = 100, min_samples_split = 2, max_depth = None, max_features = 'auto')\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train)\n",
    "rfc.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy: \", rfc.score(X_train, y_train))\n",
    "print(\"Test Accuracy: \", rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Random Forest Classifier, I received both training and test accuracies of 1.0. I am unsure of whether this means there is 100% accuracy when using when using this method or there was some sort of error made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to continue to look into the Random Forest Classifer and whether or not it would be the most effective method to use in this situation. There might also be additional classification methods that we learn in future lectures whose implementation could be beneficial to my project. Furthermore, I would like to test the accuracy of the model given unknown features (i.e. if a user did not fill out all the features for a mushroom). I want to find out whether or not it would be feasible to allow users to leave some fields blank when using the application.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
